# Concept Bottleneck Autoencoder for Stable Diffusion 1.5

This repository implements a post-hoc Concept Bottleneck Autoencoder (CB-AE) for Stable Diffusion 1.5, enabling interpretable concept prediction and semantic intervention during image generation.

## Overview

The CB-AE is inserted at the UNet bottleneck of a frozen Stable Diffusion 1.5 model. It learns to:
1. **Predict human-interpretable concepts** (e.g., Smiling, Male, Old, Mustache) from intermediate features
2. **Enable concept intervention** by modifying concept activations during generation

Key features:
- **No real labeled data required**: Training uses synthetic images generated by SD1.5 with CLIP pseudo-labels
- **Frozen backbone**: Only the lightweight CB-AE is trained; SD1.5 weights remain unchanged
- **Text-conditioned**: Works with any text prompt at inference time

## Installation

### Requirements

- Python 3.8+
- CUDA-capable GPU (16GB+ VRAM recommended)
- PyTorch 2.0+

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd cbae_sd15

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

```
torch>=2.0.0
diffusers>=0.21.0
transformers>=4.30.0
accelerate>=0.20.0
clip @ git+https://github.com/openai/CLIP.git
wandb>=0.15.0
tqdm>=4.65.0
numpy>=1.24.0
Pillow>=9.5.0
pyyaml>=6.0
```

## Project Structure

```
cbae_sd15/
├── models/
│   ├── cbae_sd15.py          # Core CB-AE model and SD1.5 wrapper
│   └── pseudo_labeler.py     # CLIP-based pseudo-labeling
├── datasets/
│   └── celeba.py             # CelebA dataset loader (optional)
├── configs/
│   └── smiling_male_old_mustache.yaml  # Example config
├── train_synthetic.py        # Main training script (synthetic data)
├── train_cbae_sd15.py        # Alternative training with real data
├── requirements.txt
└── README.md
```

## Quick Start

### Training with Synthetic Data (Recommended)

Train a CB-AE to predict four facial attributes using synthetically generated data:

```bash
python train_synthetic.py \
    --attributes Smiling Male Old Mustache \
    --batch-size 4 \
    --steps-per-epoch 500 \
    --epochs 50 \
    --wandb-project cbae-sd15
```

Or use a config file:

```bash
python train_synthetic.py --config configs/smiling_male_old_mustache.yaml
```

### Monitoring Training

Training logs are sent to [Weights & Biases](https://wandb.ai). Key metrics:
- `train/recon_loss`: Bottleneck reconstruction loss
- `train/concept_loss`: Concept alignment loss (CLIP pseudo-labels)
- `train/interv_loss`: Intervention consistency loss
- `eval/overall_accuracy`: Concept prediction accuracy

## Reproducing Paper Results

### Experiment Configuration

To reproduce our Stable Diffusion 1.5 results, use the following configuration:

```yaml
# configs/smiling_male_old_mustache.yaml

# Model settings
sd_model: "runwayml/stable-diffusion-v1-5"
img_size: 512
hidden_dim: 1024
num_layers: 4
unsupervised_dim: 64
max_timestep: 400

# Attributes to predict
attributes:
  - Smiling
  - Male
  - Old
  - Mustache

# Training settings
epochs: 50
batch_size: 4
steps_per_epoch: 500
lr: 0.0001
lr_interv: 0.0001

# Generation settings
num_inference_steps: 20
guidance_scale: 7.5
underspecified_ratio: 0.3
negative_prompt: null

# Logging settings
wandb_project: "cbae-sd15"
wandb_name: "smiling-male-old-mustache"
log_interval: 20
eval_interval: 1
save_interval: 5
log_images: true
log_images_interval: 100

# Checkpointing
checkpoint_dir: "checkpoints/smiling_male_old_mustache"

# Other settings
device: "cuda"
seed: 42
mixed_precision: false
```

### Running the Experiment

```bash
# Run training
python train_synthetic.py --config configs/smiling_male_old_mustache.yaml

# Training takes approximately 4-6 hours on a single A100 GPU
# Or 8-12 hours on a V100/RTX 3090
```

### Expected Results

After training, you should observe:

**Loss Convergence:**
| Loss | Initial | Final |
|------|---------|-------|
| Reconstruction | ~10.5 | ~7.0 |
| Concept | ~0.72 | ~0.32 |
| Intervention | ~1.0 | ~1.0 (does not converge) |

**Concept Prediction Accuracy:**
| Concept | Accuracy |
|---------|----------|
| Smiling | 88% |
| Male | 86% |
| Old | 92% |
| Mustache | 94% |
| **Overall** | **90%** |

## Training Details

### Synthetic Data Pipeline

The training pipeline generates its own data:

1. **Prompt Generation**: Random prompts are constructed with varying attributes
   - 70% attribute-rich: "a photo of a smiling elderly man with a mustache"
   - 30% underspecified: "a photo of a person" (tests generalization)

2. **Image Generation**: SD1.5 generates images using DDIM sampling (20 steps)

3. **Pseudo-labeling**: CLIP classifies generated images for each attribute

4. **CB-AE Training**: The bottleneck learns to predict CLIP labels from UNet features

### Loss Functions

| Loss | Description |
|------|-------------|
| $\mathcal{L}_{r1}$ | MSE between original and reconstructed bottleneck features |
| $\mathcal{L}_c$ | Cross-entropy between predicted concepts and CLIP pseudo-labels |
| $\mathcal{L}_{noise}$ | MSE between predicted and actual noise (maintains generation quality) |
| $\mathcal{L}_{i1}$ | External intervention loss (CLIP verifies intervened image) |
| $\mathcal{L}_{i2}$ | Internal intervention loss (cyclic consistency) |

### Architecture

The CB-AE consists of:
- **Encoder**: 4-layer MLP (20480 → 1024 → 1024 → 1024 → concept_dim)
- **Decoder**: 4-layer MLP (concept_dim → 1024 → 1024 → 1024 → 20480)
- **Concept dimension**: 2 logits per binary attribute + 64-dim unsupervised embedding

## Inference

### Loading a Trained Model

```python
import torch
from models.cbae_sd15 import SD15WithCBAE, get_concept_index

# Create model
model = SD15WithCBAE(
    n_concepts=4,
    img_size=512,
    device="cuda"
)

# Load trained weights
checkpoint = torch.load("checkpoints/smiling_male_old_mustache/best_cbae.pt")
model.cbae.load_state_dict(checkpoint["cbae_state_dict"])
model.eval()

# Attributes in order
attributes = checkpoint["attributes"]  # ["Smiling", "Male", "Old", "Mustache"]
```

### Predicting Concepts from an Image

```python
# For a generated or real image
image = ...  # [1, 3, 512, 512] tensor in range [-1, 1]

# Predict concepts
concepts = model.predict_concepts_from_image(image, text_prompt="a photo of a person")

# Get per-attribute predictions
for i, attr in enumerate(attributes):
    start, end = get_concept_index(model, i)
    logits = concepts[0, start:end]
    prob = torch.softmax(logits, dim=0)[1].item()
    print(f"{attr}: {prob:.1%}")
```

### Concept Intervention (Experimental)

```python
# Get bottleneck features
latents = model.encode_images(image)
timesteps = torch.zeros(1, device="cuda", dtype=torch.long)
text_emb = model.encode_text(["a photo of a person"])

bottleneck, cache = model.get_unet_bottleneck(latents, timesteps, text_emb)
concepts = model.cbae.encode(bottleneck)

# Intervene on "Smiling" (concept 0) - set to positive
start, end = get_concept_index(model, 0)
concepts_modified = concepts.clone()
concepts_modified[:, start:end] = torch.tensor([[0.0, 1.0]], device="cuda")

# Decode modified concepts
bottleneck_modified = model.cbae.decode(concepts_modified)
```

> **Note**: Intervention results on SD1.5 are less reliable than on unconditional DDPMs due to latent space indirection and text-conditioning entanglement. See the paper for discussion.

## Custom Attributes

To train on different attributes:

```bash
python train_synthetic.py \
    --attributes Smiling Male Eyeglasses Young Bald Heavy_Makeup \
    --batch-size 4 \
    --epochs 50
```
